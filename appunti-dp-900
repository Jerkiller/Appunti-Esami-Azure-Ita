## DP900

* lift-and-shift (es. con SQL su VM)
* ETL -> ELT
  * DW è relazionale ottimizzato per lettura. con fatti e dimensioni. DW con schema star o snowflake
  * DL invece è un bidone non strutturato, ETL verso DW , modello olap o cubo con misure, repportistica
  * OLAP dati analitici preaggregati in base alle possibili dimensioni x velocità query dopo
* Polybase - consente di creare tabelle sopra un DL
* approcci ibridi tra DW e DL, tipo con delta lake si crea un DL su cui si usa spark con estensione di tabelle e schema relazionale con API SQL
* formati
  * AVRO - apache, compresso, strutturato a righe
  * ORC - Horton Works, usato in Hive, colonnare
  * Parquet - Twitter, compresso codificato, per dati strutturati, a righe
* soluzioni OLTP (transazionali) supportano semantica ACIDa (atomicity coherence isolation durability)

## Ruoli
* DBA - sicurezza, DR, backup/restore, performance, privilegi, controllo accessi
* data engineer - spostamento dati, trasformazione, svecchiamento, privacy
* data analyst - estrae valore dai dati, crea report, identifica trend, anomalie

## Tecnologie
* ADLS data lake compatibile con hadoop o Synapse. Gen1 e Gen2. Si usa per BD, elaborati poi su HDinsight, hadoop e databricks
  * crittografia by default
  * ACL e RBAC
  * migrazione implica abilitare spazio nomi gerarchici
* Az Table
  * rowKey e partitionKey
  * documentale, non relazionale, no chiavi o vincoli integrità
* CosmosDB 99.999 (8min annui)
  * API sql, mongo, cassandra, gremlin, table
  * livelli di coerenza
  * UDF, SP e trigger
  * certificato HIPAA, FedRAMP, SOX e HITRUST
* AzSQL (OLTP) soluz PaaS flessibile x costi e scala
  * Advanced Threat Protection, AAD, MFA
* Az Synapse Analytics, motore elaboraz per BD, per task di segmentazione, predittività, analisi, ecc
  * sfruttando calcolo parallelo e semplicità di TSQL
  * usa Spark e può fare pipeline movim dati
* Stream Analytics, processing in stream
  * input: IoTHub, EventHub, Storage
  * output: blob, ADLS, SQL, Cosmos, PowerBI (in streaming), API
  * windowing per batch
* HDInisght, motore per BDC con framework spark, più complesso
  * Hadoop (HDFS, spark, Apache Hive), HBase, Kafka
  * Lib anaconda precaricate PY, cm machine learning
* Databricks (notebook serverless e cluster spark)
* DataFactory (orchestraz flussi dati, pipeline, spostamento
* DataCatalog?
* Azure Purview - servizio olistico di governance che mappa i dati e li classifica
